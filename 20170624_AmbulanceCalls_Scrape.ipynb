{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Website with ambulance data for the region of Amsterdam-Amstelland: http://www.112-nederland.nl/alerts/regiodetails.aspx?from=617475&vreg=5168&name=Amsterdam-Amstelland  \n",
    "Calls per 15 on page, page numbering goes up with 15, but you have to check last page number again before you run code  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List all pages with ambulance calls\n",
    "# Set last page number manually\n",
    "\n",
    "to_scrape_list = []\n",
    "for i in range(0,617475,15):\n",
    "    url_name = 'http://www.112-nederland.nl/alerts/regiodetails.aspx?from=' + str(i) + '&vreg=5168&name=Amsterdam-Amstelland'\n",
    "    to_scrape_list.append(url_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ambulancecalls_scraper(url_list, name_pickle):\n",
    "    \"\"\" Scrapes ambulance calls from http://www.112-nederland.nl\n",
    "    Parameters: \n",
    "        url_list: a list with all urls to scrape\n",
    "        name_pickle: str with the name of the pickle/csv to be saved\n",
    "    Returns: \n",
    "        A list of dicts with date, time, address, description of call saved in pickle (in case crash occurs)\n",
    "        and in .csv file at the end.\n",
    "    \"\"\"\n",
    "    all_calls_list = []\n",
    "    for url in url_list:\n",
    "        page = requests.get(url)\n",
    "        strainer = SoupStrainer(id='alert-ambulance')\n",
    "        soup = BeautifulSoup(page.content, 'lxml', parse_only=strainer)\n",
    "        alerts_list = soup.find_all('li')\n",
    "        del alerts_list[3] # remove advertisement\n",
    "        for alert in alerts_list[:-1]:\n",
    "            calls_dict = {}\n",
    "            heading = alert.find('h4', class_=\"media-heading\").get_text().split()\n",
    "            calls_dict['date'] = heading[0]\n",
    "            calls_dict['time'] = heading[1][:-1]\n",
    "            calls_dict['address'] = ' '.join(heading[2:-4])\n",
    "            descr = alert.find('div', itemprop=\"description\").get_text()\n",
    "            calls_dict['descr'] = re.sub('\\s+',' ',descr)\n",
    "            all_calls_list.append(calls_dict)\n",
    "            pickle.dump(all_calls_list, open(name_pickle + '.p','wb')) # in case crash occurs\n",
    "    all_calls_df = pd.DataFrame(all_calls_list, columns=['date','time','address','descr'])\n",
    "    all_calls_df.to_csv(name_pickle + '.csv', index=False) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scrape\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "ambulancecalls_scraper(to_scrape_list, 'scrape_results_page0-617475')\n",
    "\n",
    "end = time.time()\n",
    "print(\"Elapsed time in minutes: {}\".format((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for checking last scraped page\n",
    "\n",
    "part1 = pickle.load(open('scrape_results_page0-617475.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get addresses of ambulance stands\n",
    "\n",
    "url = 'https://ambulanceamsterdam.nl/regio-amsterdam-amstelland'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html5lib')\n",
    "soup_contact_list = soup.find_all(class_='contactgegeven')\n",
    "\n",
    "# 7 stands, named naam1-naam7\n",
    "list_names = ['naam' + str(i) for i in range(1,8,1)]\n",
    "\n",
    "indices = [i for i, s in enumerate(soup_contact_list) for name in list_names if name in str(s)]\n",
    "\n",
    "ambulancestands_list = []\n",
    "for index in indices:\n",
    "    ambulancestands_dict = {}\n",
    "    ambulancestands_dict['name_stand'] = soup_contact_list[index].strong.get_text()\n",
    "    ambulancestands_dict['address1'] = soup_contact_list[(index + 1)].get_text().replace(u'\\xa0', u' ')\n",
    "    ambulancestands_dict['address2'] = soup_contact_list[(index + 2)].get_text().replace(u'\\xa0', u' ')\n",
    "    ambulancestands_list.append(ambulancestands_dict) \n",
    "\n",
    "ambulancestands_df = pd.DataFrame(ambulancestands_list)\n",
    "ambulancestands_df.to_csv('.././Data/Address_ambulancestands.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
